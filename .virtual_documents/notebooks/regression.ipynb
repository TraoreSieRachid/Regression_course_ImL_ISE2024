





from pathlib import Path

import matplotlib.pyplot as plt
import missingno as msno
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objs as go
import seaborn as sns
from plotly.subplots import make_subplots
from ydata_profiling import ProfileReport 
from yellowbrick.regressor import ResidualsPlot



# Return a new path pointing to the current working directory
HOME_DIR = Path.cwd()

# create a variable for data directory
DATA_DIR = Path(HOME_DIR.parent, "data")

print(f"Work directory: {HOME_DIR} \nData directory: {DATA_DIR}")





from sklearn import linear_model

import statsmodels.formula.api as smf
from sklearn import metrics
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from statsmodels.stats.diagnostic import het_white , normal_ad
from sklearn.model_selection import validation_curve





# you must put the CSV file billets.csv in data directory, cf above cell
data = pd.read_csv(Path(DATA_DIR, "billets.csv"), sep=";")
data.shape  # data frame dimensions (nb rows, nb columns)


data_model = data.loc[lambda dfr: dfr.margin_low.notnull()]

data_model.info()


# Création des variables explicatives et cible:
# X est la liste des variables explicatives et y est la variable cible
X = data_model.loc[:, ["diagonal", "height_left", "height_right", "margin_up", "length"]]  # ou data.drop(["margin_low", "is_genuine"], axis=1)
y = data_model.loc[:, 'margin_low']


X.head()


y.head()





# Séparons nos données en un jeu d'entraînement et un jeu de test :
# 30% des données dans le jeu de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"Train set - X: {X_train.shape} ; y: {y_train.shape} \nTest set - X: {X_test.shape} ; y: {y_test.shape}")





# Standardisons les données :

std_scaler = StandardScaler().fit(X_train)  # Standardize features by removing the mean and scaling to unit variance.
X_train_std = std_scaler.transform(X_train)
X_test_std = std_scaler.transform(X_test)


X_train


X_train_std














def plot_validation_curve(model, X_train, y_train, param_name,param_range = np.logspace(-6, 6, 13), scoring='neg_mean_squared_error', cv=5):
    # Calcul des courbes de validation
    train_scores, test_scores = validation_curve(
        model, X_train, y_train, param_name=param_name, param_range=param_range,
        scoring=scoring, cv=cv
    )
    
    # Calcul des erreurs moyennes
    train_mean = -train_scores.mean(axis=1)
    test_mean = -test_scores.mean(axis=1)
    
    # Tracer la courbe de validation
    plt.figure(figsize=(8, 6))
    plt.plot(param_range, train_mean, label="Train Error", color="blue")
    plt.plot(param_range, test_mean, label="Validation Error", color="red")
    plt.xscale('log')  # Afficher en échelle logarithmique
    plt.xlabel(param_name)
    plt.ylabel('Erreur quadratique moyenne')
    plt.title(f'Courbe de validation pour {param_name} en régression Ridge')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()





def plot_validation_curve_elastic_net(X_train, y_train, l1_ratio=0.5, cv=5, scoring='neg_mean_squared_error'):
    # Définition du modèle ElasticNet avec l1_ratio fixe
    model = linear_model.ElasticNet(l1_ratio=l1_ratio)

    # Plage des valeurs de alpha à tester
    param_range = np.logspace(-6, 6, 13)  # Alpha entre 10^(-6) et 10^6

    # Calcul des scores de validation croisée
    train_scores, test_scores = validation_curve(
        model, X_train, y_train, param_name='alpha', param_range=param_range,
        scoring=scoring, cv=cv
    )

    # Calcul des moyennes d'erreurs pour chaque alpha
    train_mean = -train_scores.mean(axis=1)
    test_mean = -test_scores.mean(axis=1)

    # Tracé des courbes de validation
    plt.figure(figsize=(8, 6))
    plt.plot(param_range, train_mean, label="Train Error", color="blue")
    plt.plot(param_range, test_mean, label="Validation Error", color="red")
    plt.xscale('log')
    plt.xlabel('Alpha')
    plt.ylabel('Erreur quadratique moyenne')
    plt.title(f'Courbe de validation pour alpha en ElasticNet (l1_ratio={l1_ratio})')
    plt.legend(loc='best')
    plt.grid(True)
    plt.show()








# On crée un modèle de régression linéaire
lr = linear_model.LinearRegression()

# On entraîne ce modèle sur les données d'entrainement
lr.fit(X_train, y_train)

# On récupère l'erreur de norme 2 sur le jeu de données test comme baseline
lr_error = np.mean((lr.predict(X_test) - y_test) ** 2)

# On obtient l'erreur quadratique ci-dessous
print(lr_error)


print("Intercept:", lr.intercept_)
print("Coefficients:", lr.coef_)





# Instanciation:
reg_lin = smf.ols("margin_low ~ diagonal + height_left + height_right + margin_up + length", data=data_model)

# Calculs:
res_lin = reg_lin.fit()
res_lin.summary()





import matplotlib.pyplot as plt


y_train_pred = lr.predict(X_train)
y_test_pred = lr.predict(X_test)





fig = make_subplots(rows=1, cols=2, shared_yaxes=False)

for idx, (name, y_true, y_pred) in enumerate([("Train", y_train, y_train_pred),
                                              ("Test", y_test, y_test_pred),
                                             ]):
    col = idx + 1
    fig.add_trace(go.Scatter(x=y_true, y=y_pred, mode="markers", name=name), row=1, col=col)

    fig.add_shape(type="line",
              x0=y_true.min(), x1=y_true.max(),
              y0=y_true.min(), y1=y_true.max(),
              row=1, col=col)
    
fig.show()





res_viz = ResidualsPlot(lr,
                        is_fitted="auto",
                        qqplot=True,
                        hist=False,
                        train_color="blue",
                        test_color="red",
                       )
res_viz.fit(X_train, y_train)
res_viz.score(X_test, y_test)
res_viz.show(clear_figure=True);





# La moyenne des residus est:
res_lin.resid.mean()





# Normalite:
print("La p-value du test d'Aderson-Darling vaux", normal_ad(res_lin.resid)[1])








# Homoscedasticite:
white_test = het_white(res_lin.resid, res_lin.model.exog)
labels_white_test = ["Test Statistic", "p-value", "F-Statistic", "F-Test p-value"]
print(dict(zip(labels_white_test, white_test)))








# Check for Multicollinearity
# Variance Inflation Factor (VIF)
r_square_test = (metrics.root_mean_squared_error(y_test, lr.predict(X_test)))**2
vif = 1 / (1 - r_square_test)
vif





# On crée un modèle de régression linéaire
ridge = linear_model.Ridge(alpha=1.)

# On entraîne ce modèle sur les données d'entrainement
ridge.fit(X_train, y_train)

# On récupère l'erreur de norme 2 sur le jeu de données test
ridge_error = np.mean((ridge.predict(X_test) - y_test) ** 2)


# On obtient l'erreur quadratique ci-dessous
print(ridge_error)


mse = metrics.mean_squared_error(ridge.predict(X_test), y_test)
rmse = np.sqrt(mse)  # metrics.mean_squared_error(ridge.predict(X_test), y_test, squared=False)
mae = metrics.mean_absolute_error(ridge.predict(X_test), y_test)

mse, rmse, mae





def train_model(model, x_train, y_train, x_test, y_test):
    # On entraîne ce modèle sur les données d'entrainement
    model.fit(x_train, y_train)
    
    # On récupère l'erreur de norme 2 sur le jeu de données train
    error_train = np.mean((model.predict(x_train) - y_train) ** 2)

    # On récupère l'erreur de norme 2 sur le jeu de données test
    error_test = np.mean((model.predict(x_test) - y_test) ** 2)

    # On obtient l'erreur quadratique ci-dessous
    print(f"Model error: {round(error_test, 5)}")
    return {"estimator": model, "error_train": error_train, "error_test": error_test}


# Train and evaluate ridge regression
ridge_error = train_model(model=linear_model.Ridge(alpha=1.),
                          x_train=X_train, y_train=y_train,
                          x_test=X_test, y_test=y_test)





# Initialiser le modèle Ridge
Ridge = linear_model.Ridge()
plot_validation_curve(Ridge, X_train, y_train, param_name='alpha')


# Plage des valeurs d'alpha à tester
param_range = np.logspace(-6, 1, 7)  # Intervalle de 10^(-6) à 10^(0)

# Création du dictionnaire des paramètres pour GridSearchCV
param_grid = {'alpha': param_range}

# Instanciation du modèle Ridge
ridge_model = linear_model.Ridge()

# Configuration de GridSearchCV
grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, 
                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)

# Entraînement avec les données d'entraînement
grid_search.fit(X_train, y_train)

# Affichage des meilleurs paramètres et du score associé
print("Meilleur paramètre alpha :", grid_search.best_params_['alpha'])
print("Meilleur score (erreur quadratique moyenne négative) :", grid_search.best_score_)



# Train and evaluate ridge regression
ridge_error_opti = train_model(model=linear_model.Ridge(alpha=grid_search.best_params_['alpha']),
                          x_train=X_train, y_train=y_train,
                          x_test=X_test, y_test=y_test)





# Train and evaluate ridge regression
lasso_error = train_model(model=linear_model.Lasso(fit_intercept=True, alpha=1.,),
                          x_train=X_train_std, y_train=y_train,
                          x_test=X_test_std, y_test=y_test)


Lasso = linear_model.Lasso()
plot_validation_curve(Lasso, X_train, y_train, param_name='alpha')


# Initialiser le modèle Lasso
lasso = linear_model.Lasso()

# Plage des valeurs d'alpha à tester
param_range = np.logspace(-6, -3, 7)  # Intervalle de 10^(-6) à 10^(-3)

# Création du dictionnaire des paramètres pour GridSearchCV
param_grid = {'alpha': param_range}
# Utiliser la validation croisée pour optimiser alpha
lasso_grid_search = GridSearchCV(lasso, param_grid, cv=5)
lasso_grid_search.fit(X_train, y_train)

# Meilleur paramètre alpha
print(f"Best alpha for Lasso: {lasso_grid_search.best_params_['alpha']}")



# Train and evaluate ridge regression
lasso_error = train_model(model=linear_model.Lasso(fit_intercept=True, alpha=lasso_grid_search.best_params_['alpha'],),
                          x_train=X_train_std, y_train=y_train,
                          x_test=X_test_std, y_test=y_test)








# Train and evaluate ridge regression
elastic_error = train_model(model=linear_model.ElasticNet(fit_intercept=True, alpha=1., l1_ratio=.5),
                            x_train=X_train_std, y_train=y_train,
                            x_test=X_test_std, y_test=y_test)





# Utilisation de la fonction
plot_validation_curve_elastic_net(X_train, y_train, l1_ratio=0.5)


# Définir une grille de paramètres pour ElasticNet (alpha et l1_ratio)
param_grid_en = {'alpha': [0.0001,0.001,0.01,0.1], 'l1_ratio': [0.1, 0.5, 0.9, 1]}

# Initialiser le modèle ElasticNet
elasticnet = linear_model.ElasticNet()

# Utiliser la validation croisée pour optimiser alpha et l1_ratio
en_grid_search = GridSearchCV(elasticnet, param_grid_en, cv=5)
en_grid_search.fit(X_train, y_train)

# Meilleurs paramètres alpha et l1_ratio
print(f"Best alpha for ElasticNet: {en_grid_search.best_params_['alpha']}")
print(f"Best l1_ratio for ElasticNet: {en_grid_search.best_params_['l1_ratio']}")



# Train and evaluate ridge regression
elastic_error = train_model(model=linear_model.ElasticNet(fit_intercept=True, alpha=en_grid_search.best_params_['alpha'], 
                                                          l1_ratio=en_grid_search.best_params_['l1_ratio']),
                            x_train=X_train_std, y_train=y_train,
                            x_test=X_test_std, y_test=y_test)














from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, GridSearchCV

from sklearn import set_config

set_config(display="diagram", print_changed_only=True)


# définition de Pipeline de régression avec Pipeline (c'est à nous de données les noms de chaque étape du workflow)
reg_pipe = Pipeline(steps=[("scaler", MinMaxScaler()),
                           ("regressor", linear_model.LinearRegression()),
                          ]
                   )
# ou via make_pipeline (la seule différence est que make_pipeline génère automatiquement des noms pour les étapes).
# reg_pipe = make_pipeline(StandardScaler(), linear_model.LinearRegression())
reg_pipe





# Train and evaluate ridge regression
lr_results = train_model(model=reg_pipe,
                       x_train=X_train, y_train=y_train,
                       x_test=X_test, y_test=y_test)

lr_pipe = lr_results["estimator"]


### Métriques d'évaluation


METRICS = [metrics.r2_score,
           metrics.mean_squared_error,
           metrics.mean_absolute_percentage_error,
           metrics.max_error,
          ]


# get performances in train & test
get_all_performances(value_train=(y_train, lr_pipe.predict(X_train)),
                     values_test=(y_test, lr_pipe.predict(X_test)),
                     metrics=METRICS
                    )








from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, GridSearchCV


# Pour lister les noms des métriques à fournir à scoring, vous pouvez consulter le site de sklearn ou 
# éxecuter dans une cellule de code: sklearn.metrics.SCORERS.
# Avec cross_validation, il est possible d'optimiser le modèle avec plusieurs métriques d'évaluation.

scores = cross_validate(reg_pipe, X_train, y_train, cv=5,
                        scoring=["r2"],  # ["r2", "neg_root_mean_squared_error"]
                        return_train_score=True,
                        verbose=True,
                       )
scores


pd.DataFrame(scores)


cross_val_score(reg_pipe, X_train, y_train, cv=5, scoring="r2")


grid_search.predict(X_test)


df_feature_importance = pd.DataFrame(reg_pipe[-1].coef_, columns=["coef"], index=grid_search.feature_names_in_)
print(f"Shape: {df_feature_importance.shape}")
df_feature_importance.head()


# top30 most important features
(df_feature_importance
 .sort_values("coef", key=lambda v: abs(v), ascending=True)
 .plot(kind="barh", figsize=(10, 7))
)
plt.title("Linear model")
plt.axvline(x=0, color='.6')
plt.subplots_adjust(left=.3);






rig_pipe = Pipeline(steps=[("scaler", MinMaxScaler()),
                           ("regressor", ridge_error_opti),
                          ]
                   )


df_feature_importance = pd.DataFrame(reg_pipe[-1].coef_, columns=["coef"], index=grid_search.feature_names_in_)
print(f"Shape: {df_feature_importance.shape}")
df_feature_importance.head()



# top30 most important features
(df_feature_importance
 .sort_values("coef", key=lambda v: abs(v), ascending=True)
 .plot(kind="barh", figsize=(10, 7))
)
plt.title("Linear model")
plt.axvline(x=0, color='.6')
plt.subplots_adjust(left=.3);





lasso_pipe = Pipeline(steps=[("scaler", MinMaxScaler()),
                           ("regressor",lasso_error)
                          ]
                   )


df_feature_importance = pd.DataFrame(reg_pipe[-1].coef_, columns=["coef"], index=grid_search.feature_names_in_)
print(f"Shape: {df_feature_importance.shape}")
df_feature_importance.head()



# top30 most important features
(df_feature_importance
 .sort_values("coef", key=lambda v: abs(v), ascending=True)
 .plot(kind="barh", figsize=(10, 7))
)
plt.title("Linear model")
plt.axvline(x=0, color='.6')
plt.subplots_adjust(left=.3);





lasso_pipe = Pipeline(steps=[("scaler", MinMaxScaler()),
                           ("regressor", elastic_error),
                          ]
                   )




df_feature_importance = pd.DataFrame(reg_pipe[-1].coef_, columns=["coef"], index=grid_search.feature_names_in_)
print(f"Shape: {df_feature_importance.shape}")
df_feature_importance.head()


# top30 most important features
(df_feature_importance
 .sort_values("coef", key=lambda v: abs(v), ascending=True)
 .plot(kind="barh", figsize=(10, 7))
)
plt.title("Linear model")
plt.axvline(x=0, color='.6')
plt.subplots_adjust(left=.3);
